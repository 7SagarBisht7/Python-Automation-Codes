{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Lj5Worro77V",
        "outputId": "5583718b-8708-43e2-88fd-e72c870086dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scrapy\n",
            "  Using cached scrapy-2.14.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (1.6.0)\n",
            "Requirement already satisfied: cryptography>=37.0.0 in /usr/local/lib/python3.12/dist-packages (from scrapy) (43.0.3)\n",
            "Collecting cssselect>=0.9.1 (from scrapy)\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from scrapy) (0.7.1)\n",
            "Collecting itemadapter>=0.1.0 (from scrapy)\n",
            "  Downloading itemadapter-0.13.1-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting itemloaders>=1.0.1 (from scrapy)\n",
            "  Downloading itemloaders-1.3.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: lxml>=4.6.4 in /usr/local/lib/python3.12/dist-packages (from scrapy) (6.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from scrapy) (25.0)\n",
            "Collecting parsel>=1.5.0 (from scrapy)\n",
            "  Downloading parsel-1.10.0-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting protego>=0.1.15 (from scrapy)\n",
            "  Downloading protego-0.5.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting pydispatcher>=2.0.5 (from scrapy)\n",
            "  Downloading PyDispatcher-2.0.7-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: pyopenssl>=22.0.0 in /usr/local/lib/python3.12/dist-packages (from scrapy) (24.2.1)\n",
            "Collecting queuelib>=1.4.2 (from scrapy)\n",
            "  Downloading queuelib-1.8.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting service-identity>=18.1.0 (from scrapy)\n",
            "  Downloading service_identity-24.2.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting tldextract (from scrapy)\n",
            "  Downloading tldextract-5.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting twisted<=25.5.0,>=21.7.0 (from scrapy)\n",
            "  Downloading twisted-25.5.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting w3lib>=1.17.0 (from scrapy)\n",
            "  Downloading w3lib-2.3.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting zope-interface>=5.1.0 (from scrapy)\n",
            "  Downloading zope_interface-8.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=37.0.0->scrapy) (2.0.0)\n",
            "Collecting jmespath>=0.9.5 (from itemloaders>=1.0.1->scrapy)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from service-identity>=18.1.0->scrapy) (25.4.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.12/dist-packages (from service-identity>=18.1.0->scrapy) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.12/dist-packages (from service-identity>=18.1.0->scrapy) (0.4.2)\n",
            "Collecting automat>=24.8.0 (from twisted<=25.5.0,>=21.7.0->scrapy)\n",
            "  Downloading automat-25.4.16-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting constantly>=15.1 (from twisted<=25.5.0,>=21.7.0->scrapy)\n",
            "  Downloading constantly-23.10.4-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting hyperlink>=17.1.1 (from twisted<=25.5.0,>=21.7.0->scrapy)\n",
            "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting incremental>=24.7.0 (from twisted<=25.5.0,>=21.7.0->scrapy)\n",
            "  Downloading incremental-24.11.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.12/dist-packages (from twisted<=25.5.0,>=21.7.0->scrapy) (4.15.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from tldextract->scrapy) (3.11)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from tldextract->scrapy) (2.32.4)\n",
            "Collecting requests-file>=1.4 (from tldextract->scrapy)\n",
            "  Downloading requests_file-3.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.12/dist-packages (from tldextract->scrapy) (3.20.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=37.0.0->scrapy) (2.23)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.1.0->tldextract->scrapy) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2026.1.4)\n",
            "Downloading scrapy-2.14.1-py3-none-any.whl (331 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.7/331.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading itemadapter-0.13.1-py3-none-any.whl (18 kB)\n",
            "Downloading itemloaders-1.3.2-py3-none-any.whl (12 kB)\n",
            "Downloading parsel-1.10.0-py2.py3-none-any.whl (17 kB)\n",
            "Downloading protego-0.5.0-py3-none-any.whl (10 kB)\n",
            "Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
            "Downloading queuelib-1.8.0-py3-none-any.whl (13 kB)\n",
            "Downloading service_identity-24.2.0-py3-none-any.whl (11 kB)\n",
            "Downloading twisted-25.5.0-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading w3lib-2.3.1-py3-none-any.whl (21 kB)\n",
            "Downloading zope_interface-8.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (264 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.9/264.9 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.3.1-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.9/105.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading automat-25.4.16-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading constantly-23.10.4-py3-none-any.whl (13 kB)\n",
            "Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading incremental-24.11.0-py3-none-any.whl (21 kB)\n",
            "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading requests_file-3.0.1-py2.py3-none-any.whl (4.5 kB)\n",
            "Installing collected packages: pydispatcher, zope-interface, w3lib, queuelib, protego, jmespath, itemadapter, incremental, hyperlink, cssselect, constantly, automat, twisted, requests-file, parsel, tldextract, service-identity, itemloaders, scrapy\n",
            "Successfully installed automat-25.4.16 constantly-23.10.4 cssselect-1.3.0 hyperlink-21.0.0 incremental-24.11.0 itemadapter-0.13.1 itemloaders-1.3.2 jmespath-1.0.1 parsel-1.10.0 protego-0.5.0 pydispatcher-2.0.7 queuelib-1.8.0 requests-file-3.0.1 scrapy-2.14.1 service-identity-24.2.0 tldextract-5.3.1 twisted-25.5.0 w3lib-2.3.1 zope-interface-8.2\n"
          ]
        }
      ],
      "source": [
        "!pip install scrapy nest_asyncio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scrapy\n",
        "import nest_asyncio\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "yfAYX-krpAjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InfiniteScrollSpider(scrapy.Spider):\n",
        "  name = \"Infinite_scroll\"\n",
        "  def start_requests(self):\n",
        "    for page in range(1,6):\n",
        "      url = f\"https://infinite-scroll.com/demo/full-page/page/{page}\"\n",
        "      print(f\"Requesting URL: {url}\")\n",
        "      yield scrapy.Request(url=url, callback=self.parse)\n",
        "  def parse(self, response):\n",
        "    print(f\"Scraped Page: {response.url}\")\n",
        "    posts = response.css(\"article.post\")\n",
        "    for post in posts:\n",
        "      title = post.css(\"h2::title\").get()\n",
        "      print(f\"Post Title: {title}\")\n",
        "      print(\"-\"*50)\n",
        "\n",
        "process = CrawlerProcess(settings={\n",
        "    \"LOG_LEVEL\":\"ERROR\"\n",
        "})\n",
        "process.crawl(InfiniteScrollSpider)\n",
        "process.start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcY-8a_tpg_t",
        "outputId": "9097f9db-cee0-4a92-9d09-287cb27b849c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:scrapy.utils.log:Scrapy 2.14.1 started (bot: scrapybot)\n",
            "INFO:scrapy.utils.log:Versions:\n",
            "{'lxml': '6.0.2',\n",
            " 'libxml2': '2.14.6',\n",
            " 'cssselect': '1.3.0',\n",
            " 'parsel': '1.10.0',\n",
            " 'w3lib': '2.3.1',\n",
            " 'Twisted': '25.5.0',\n",
            " 'Python': '3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]',\n",
            " 'pyOpenSSL': '24.2.1 (OpenSSL 3.3.2 3 Sep 2024)',\n",
            " 'cryptography': '43.0.3',\n",
            " 'Platform': 'Linux-6.6.105+-x86_64-with-glibc2.35'}\n",
            "DEBUG:scrapy.crawler:Using CrawlerProcess\n",
            "INFO:scrapy.addons:Enabled addons:\n",
            "[]\n",
            "DEBUG:scrapy.utils.log:Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "DEBUG:scrapy.utils.log:Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "INFO:scrapy.extensions.telnet:Telnet Password: c5ef6b441e40f266\n",
            "INFO:scrapy.middleware:Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.logcount.LogCount',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "INFO:scrapy.crawler:Overridden settings:\n",
            "{'LOG_LEVEL': 'ERROR'}\n",
            "INFO:scrapy.middleware:Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "INFO:scrapy.middleware:Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.start.StartSpiderMiddleware',\n",
            " 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "INFO:scrapy.middleware:Enabled item pipelines:\n",
            "[]\n",
            "INFO:scrapy.core.engine:Spider opened\n",
            "/usr/local/lib/python3.12/dist-packages/scrapy/core/spidermw.py:490: ScrapyDeprecationWarning: __main__.InfiniteScrollSpider defines the deprecated start_requests() method. start_requests() has been deprecated in favor of a new method, start(), to support asynchronous code execution. start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace start_requests() with start(); note that start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when overriding start_requests() in a spider class, override start() as well; you can use super() to reuse the inherited start() implementation without copy-pasting. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html\n",
            "  warn(\n",
            "INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "INFO:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6023\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requesting URL: https://infinite-scroll.com/demo/full-page/page/1\n",
            "Requesting URL: https://infinite-scroll.com/demo/full-page/page/2\n",
            "Requesting URL: https://infinite-scroll.com/demo/full-page/page/3\n",
            "Requesting URL: https://infinite-scroll.com/demo/full-page/page/4\n",
            "Requesting URL: https://infinite-scroll.com/demo/full-page/page/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:scrapy.core.engine:Crawled (404) <GET https://infinite-scroll.com/demo/full-page/page/1> (referer: None)\n",
            "DEBUG:scrapy.core.engine:Crawled (404) <GET https://infinite-scroll.com/demo/full-page/page/2> (referer: None)\n",
            "DEBUG:scrapy.core.engine:Crawled (404) <GET https://infinite-scroll.com/demo/full-page/page/5> (referer: None)\n",
            "DEBUG:scrapy.core.engine:Crawled (404) <GET https://infinite-scroll.com/demo/full-page/page/4> (referer: None)\n",
            "DEBUG:scrapy.core.engine:Crawled (404) <GET https://infinite-scroll.com/demo/full-page/page/3> (referer: None)\n",
            "INFO:scrapy.spidermiddlewares.httperror:Ignoring response <404 https://infinite-scroll.com/demo/full-page/page/1>: HTTP status code is not handled or not allowed\n",
            "INFO:scrapy.spidermiddlewares.httperror:Ignoring response <404 https://infinite-scroll.com/demo/full-page/page/2>: HTTP status code is not handled or not allowed\n",
            "INFO:scrapy.spidermiddlewares.httperror:Ignoring response <404 https://infinite-scroll.com/demo/full-page/page/5>: HTTP status code is not handled or not allowed\n",
            "INFO:scrapy.spidermiddlewares.httperror:Ignoring response <404 https://infinite-scroll.com/demo/full-page/page/4>: HTTP status code is not handled or not allowed\n",
            "INFO:scrapy.spidermiddlewares.httperror:Ignoring response <404 https://infinite-scroll.com/demo/full-page/page/3>: HTTP status code is not handled or not allowed\n",
            "INFO:scrapy.core.engine:Closing spider (finished)\n",
            "INFO:scrapy.statscollectors:Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 1255,\n",
            " 'downloader/request_count': 5,\n",
            " 'downloader/request_method_count/GET': 5,\n",
            " 'downloader/response_bytes': 25518,\n",
            " 'downloader/response_count': 5,\n",
            " 'downloader/response_status_count/404': 5,\n",
            " 'elapsed_time_seconds': 0.486254,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2026, 1, 19, 10, 0, 22, 622653, tzinfo=datetime.timezone.utc),\n",
            " 'httpcompression/response_bytes': 100855,\n",
            " 'httpcompression/response_count': 5,\n",
            " 'httperror/response_ignored_count': 5,\n",
            " 'httperror/response_ignored_status_count/404': 5,\n",
            " 'items_per_minute': None,\n",
            " 'memusage/max': 214290432,\n",
            " 'memusage/startup': 214290432,\n",
            " 'response_received_count': 5,\n",
            " 'responses_per_minute': None,\n",
            " 'scheduler/dequeued': 5,\n",
            " 'scheduler/dequeued/memory': 5,\n",
            " 'scheduler/enqueued': 5,\n",
            " 'scheduler/enqueued/memory': 5,\n",
            " 'start_time': datetime.datetime(2026, 1, 19, 10, 0, 22, 136399, tzinfo=datetime.timezone.utc)}\n",
            "INFO:scrapy.core.engine:Spider closed (finished)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qBaZ438Up7sr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}